#!/usr/bin/env python3

# OUTPUT CONFIGURATION - CHOOSE WHICH LINES TO DISPLAY

# Set which lines to display (True = show, False = hide)
SHOW_LINE1 = True   # [Sonnet 4] | ğŸŒ¿ main M2 +1 | ğŸ“ statusline | ğŸ’¬ 254
SHOW_LINE2 = True   # Compact: 91.8K/160.0K â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–’â–’ 58% â™»ï¸ 99% cached ğŸ’° $0.031
SHOW_LINE3 = True   # Session: 1h15m/5h â–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’ 25% 09:15 (08:00 to 13:00)
SHOW_LINE4 = True   # Burn: 14.0M (Rate: 321K t/m) â–â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–‡â–†â–…â–„â–ƒâ–‚â–â–â–â–‚â–ƒ

# Alternative quick configurations (uncomment one to use):
# SHOW_LINE1, SHOW_LINE2, SHOW_LINE3, SHOW_LINE4 = True, True, False, False   # Only lines 1-2
# SHOW_LINE1, SHOW_LINE2, SHOW_LINE3, SHOW_LINE4 = False, True, True, True    # Skip line 1
# SHOW_LINE1, SHOW_LINE2, SHOW_LINE3, SHOW_LINE4 = False, False, True, True   # Only lines 3-4

# IMPORTS AND SYSTEM CODE

import json
import sys
import os
import subprocess
import argparse
import shutil
import re
import unicodedata
from pathlib import Path
from datetime import datetime, timedelta, timezone, date
import time
from collections import defaultdict

# CONSTANTS

# Token compaction threshold - FALLBACK VALUE ONLY
# Dynamic value is now calculated from API: context_window_size * 0.8
# This constant is kept for backwards compatibility if API data is unavailable
COMPACTION_THRESHOLD = 200000 * 0.8  # 80% of 200K tokens (fallback)

# TWO DISTINCT TOKEN CALCULATION SYSTEMS

# This application uses TWO completely separate token calculation systems:

# ğŸ—œï¸ COMPACT LINE SYSTEM (Conversation Compaction)
# ==============================================
# Purpose: Tracks current conversation progress toward compaction threshold
# Data Source: Current conversation tokens (until 160K compaction limit)
# Scope: Single conversation, monitors compression timing
# Calculation: block_stats['total_tokens'] from detect_five_hour_blocks()
# Display: Compact line (Line 2) - "118.1K/160.0K â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’â–’ 74%"
# Range: 0-200K tokens (until conversation gets compressed)
# Reset Point: When conversation gets compacted/compressed

# ğŸ• SESSION WINDOW SYSTEM (Session Management)
# ===================================================
# Purpose: Tracks usage periods
# Data Source: Messages within usage windows
# Scope: Usage period tracking
# Calculation: calculate_tokens_since_time() with 5-hour window start
# Display: Session line (Line 3) + Burn line (Line 4)
# Range: usage window scope with real-time burn rate
# Reset Point: Every 5 hours per usage limits

# âš ï¸  CRITICAL RULES:
# 1. COMPACT = conversation compaction monitoring (160K threshold)
# 2. SESSION/BURN = usage window tracking
# 3. These track DIFFERENT concepts: compression vs usage periods
# 4. Compact = compression timing, Session = official usage window

# ANSI color codes optimized for black backgrounds
class Colors:
    _colors = {
        'BRIGHT_CYAN': '\033[1;96m',
        'BRIGHT_BLUE': '\033[1;94m', 
        'BRIGHT_MAGENTA': '\033[1;95m',
        'BRIGHT_GREEN': '\033[1;92m',
        'BRIGHT_YELLOW': '\033[1;93m',
        'BRIGHT_RED': '\033[1;95m',
        'BRIGHT_WHITE': '\033[1;97m',
        'LIGHT_GRAY': '\033[1;97m',
        'DIM': '\033[1;97m',
        'BOLD': '\033[1m',
        'BLINK': '\033[5m',
        'BG_RED': '\033[41m',
        'BG_YELLOW': '\033[43m',
        'RESET': '\033[0m'
    }
    
    def __getattr__(self, name):
        if os.environ.get('NO_COLOR') or os.environ.get('STATUSLINE_NO_COLOR'):
            return ''
        return self._colors.get(name, '')

# Create single instance
Colors = Colors()

# ========================================
# TERMINAL WIDTH UTILITIES
# ========================================

def strip_ansi(text):
    """ANSIã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚³ãƒ¼ãƒ‰ã‚’é™¤å»"""
    return re.sub(r'\x1b\[[0-9;]*m', '', text)

def get_display_width(text):
    """è¡¨ç¤ºå¹…ã‚’è¨ˆç®—ï¼ˆçµµæ–‡å­—/CJKå¯¾å¿œï¼‰

    ANSIã‚³ãƒ¼ãƒ‰ã‚’é™¤å»ã—ã€å„æ–‡å­—ã®è¡¨ç¤ºå¹…ã‚’è¨ˆç®—ã€‚
    East Asian Width ãŒ 'W' (Wide) ã¾ãŸã¯ 'F' (Fullwidth) ã®æ–‡å­—ã¯å¹…2ã€ãã‚Œä»¥å¤–ã¯å¹…1ã€‚
    """
    clean = strip_ansi(text)
    width = 0
    for char in clean:
        ea = unicodedata.east_asian_width(char)
        width += 2 if ea in ('W', 'F') else 1
    return width

def get_terminal_width():
    """ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å¹…ã‚’å–å¾—ï¼ˆå®‰å…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰

    å„ªå…ˆé †ä½:
    1. COLUMNSç’°å¢ƒå¤‰æ•°ï¼ˆæ˜ç¤ºçš„æŒ‡å®šï¼‰
    2. tmux paneå¹…ï¼ˆtmuxç’°å¢ƒã®å ´åˆï¼‰
    3. tput colsï¼ˆTTYä¸è¦ï¼‰
    4. shutil.get_terminal_size()ï¼ˆTTYå¿…è¦ï¼‰
    5. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ80

    Returns:
        int: ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å¹…ï¼ˆå³ç«¯1æ–‡å­—å•é¡Œå¯¾ç­–ã§-1ï¼‰
    """
    try:
        # 1. ç’°å¢ƒå¤‰æ•°COLUMNSã‚’æœ€å„ªå…ˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ãƒ»æ˜ç¤ºçš„æŒ‡å®šï¼‰
        if 'COLUMNS' in os.environ:
            try:
                return int(os.environ['COLUMNS']) - 1
            except ValueError:
                pass

        # 2. tmuxç’°å¢ƒã®å ´åˆã€paneå¹…ã‚’å–å¾—
        if 'TMUX' in os.environ:
            try:
                result = subprocess.run(
                    ['tmux', 'display-message', '-p', '#{pane_width}'],
                    capture_output=True, text=True, timeout=1
                )
                if result.returncode == 0 and result.stdout.strip().isdigit():
                    return int(result.stdout.strip()) - 1
            except (subprocess.TimeoutExpired, FileNotFoundError, OSError):
                pass

        # 3. tput colsï¼ˆTTYä¸è¦ã€$TERMã‹ã‚‰å–å¾—ï¼‰
        try:
            result = subprocess.run(
                ['tput', 'cols'],
                capture_output=True, text=True, timeout=1
            )
            if result.returncode == 0 and result.stdout.strip().isdigit():
                return int(result.stdout.strip()) - 1
        except (subprocess.TimeoutExpired, FileNotFoundError, OSError):
            pass

        # 4. shutil.get_terminal_size()ï¼ˆTTYå¿…è¦ï¼‰
        if sys.stdout.isatty():
            size = shutil.get_terminal_size()
            if size.columns > 0:
                return size.columns - 1

    except (OSError, AttributeError):
        pass

    return 80  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

def get_display_mode(width):
    """ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å¹…ã‹ã‚‰ãƒ¢ãƒ¼ãƒ‰ã‚’æ±ºå®šï¼ˆãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹å¯¾å¿œï¼‰

    | ãƒ¢ãƒ¼ãƒ‰ | å¹… | è¡¨ç¤ºå†…å®¹ |
    |--------|-----|---------|
    | full | >= 72 | 4è¡Œãƒ»å…¨é …ç›®ãƒ»è£…é£¾ã‚ã‚Š |
    | compact | 55-71 | 4è¡Œãƒ»ãƒ©ãƒ™ãƒ«çŸ­ç¸®ãƒ»è£…é£¾å‰Šæ¸› |
    | tight | 45-54 | 2-3è¡Œãƒ»é‡è¦é …ç›®ã®ã¿ |
    | minimal | < 45 | 1è¡Œã‚µãƒãƒª |

    Args:
        width: ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å¹…
    Returns:
        str: 'full', 'compact', 'tight', or 'minimal'
    """
    if width >= 72:      # Full: 70 + 2ï¼ˆãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹ï¼‰
        return 'full'
    elif width >= 55:
        return 'compact'
    elif width >= 45:
        return 'tight'
    else:
        return 'minimal'

def get_total_tokens(usage_data):
    """Calculate total tokens from usage data (UNIVERSAL HELPER) - external tool compatible
    
    Used by session/burn line systems for usage window tracking.
    Sums all token types: input + output + cache_creation + cache_read
    
    CRITICAL FIX: Implements external tool compatible logic to avoid double-counting
    
    Args:
        usage_data: Token usage dictionary from assistant message
    Returns:
        int: Total tokens across all types
    """
    if not usage_data:
        return 0
    
    # Handle both field name variations
    input_tokens = usage_data.get('input_tokens', 0)
    output_tokens = usage_data.get('output_tokens', 0)
    
    # Cache creation tokens - external tool compatible logic
    # Use direct field first, fallback to nested if not present
    if 'cache_creation_input_tokens' in usage_data:
        cache_creation = usage_data['cache_creation_input_tokens']
    elif 'cache_creation' in usage_data and isinstance(usage_data['cache_creation'], dict):
        cache_creation = usage_data['cache_creation'].get('ephemeral_5m_input_tokens', 0)
    else:
        cache_creation = (
            usage_data.get('cacheCreationInputTokens', 0) or
            usage_data.get('cacheCreationTokens', 0)
        )
    
    # Cache read tokens - external tool compatible logic  
    if 'cache_read_input_tokens' in usage_data:
        cache_read = usage_data['cache_read_input_tokens']
    elif 'cache_read' in usage_data and isinstance(usage_data['cache_read'], dict):
        cache_read = usage_data['cache_read'].get('ephemeral_5m_input_tokens', 0)
    else:
        cache_read = (
            usage_data.get('cacheReadInputTokens', 0) or
            usage_data.get('cacheReadTokens', 0)
        )
    
    return input_tokens + output_tokens + cache_creation + cache_read

def format_token_count(tokens):
    """Format token count for display"""
    if tokens >= 1000000:
        return f"{tokens / 1000000:.1f}M"
    elif tokens >= 1000:
        return f"{tokens / 1000:.1f}K"
    return str(tokens)

def format_token_count_short(tokens):
    """Format token count for display (3 significant digits)"""
    if tokens >= 1000000:
        val = tokens / 1000000
        if val >= 100:
            return f"{round(val)}M"      # 100M, 200M
        else:
            return f"{val:.1f}M"         # 14.0M, 1.5M
    elif tokens >= 1000:
        val = tokens / 1000
        if val >= 100:
            return f"{round(val)}K"      # 332K, 500K
        else:
            return f"{val:.1f}K"         # 14.0K, 99.5K
    return str(tokens)

def convert_utc_to_local(utc_time):
    """Convert UTC timestamp to local time (common utility)"""
    if hasattr(utc_time, 'tzinfo') and utc_time.tzinfo:
        return utc_time.astimezone()
    else:
        # UTC timestamp without timezone info
        utc_with_tz = utc_time.replace(tzinfo=timezone.utc)
        return utc_with_tz.astimezone()

def convert_local_to_utc(local_time):
    """Convert local timestamp to UTC (common utility)"""
    if hasattr(local_time, 'tzinfo') and local_time.tzinfo:
        return local_time.astimezone(timezone.utc)
    else:
        # Local timestamp without timezone info
        return local_time.replace(tzinfo=timezone.utc)

def get_percentage_color(percentage):
    """Get color based on percentage threshold"""
    if percentage >= 90:
        return '\033[1;91m'  # å…ƒã®èµ¤è‰²
    elif percentage >= 70:
        return Colors.BRIGHT_YELLOW
    return Colors.BRIGHT_GREEN

def calculate_dynamic_padding(compact_text, session_text):
    """Calculate dynamic padding to align progress bars
    
    Args:
        compact_text: Text part of compact line (e.g., "Compact: 111.6K/160.0K")
        session_text: Text part of session line (e.g., "Session: 3h26m/5h")
    
    Returns:
        str: Padding spaces for session line
    """
    # Remove ANSI color codes for accurate length calculation
    import re
    clean_compact = re.sub(r'\x1b\[[0-9;]*m', '', compact_text)
    clean_session = re.sub(r'\x1b\[[0-9;]*m', '', session_text)
    
    compact_len = len(clean_compact)
    session_len = len(clean_session)
    
    
    
    if session_len < compact_len:
        return ' ' * (compact_len - session_len + 1)  # +1 for visual adjustment
    else:
        return ' '

def get_progress_bar(percentage, width=20, show_current_segment=False):
    """Create a visual progress bar with optional current segment highlighting"""
    filled = int(width * percentage / 100)
    empty = width - filled
    
    color = get_percentage_color(percentage)
    
    if show_current_segment and filled < width:
        # å®Œäº†æ¸ˆã¿ã¯å…ƒã®è‰²ã‚’ä¿æŒã€ç¾åœ¨é€²è¡Œä¸­ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ã¿ç‰¹åˆ¥è¡¨ç¤º
        completed_bar = color + 'â–ˆ' * filled if filled > 0 else ''
        current_bar = Colors.BRIGHT_WHITE + 'â–“' + Colors.RESET  # ç™½ãç‚¹æ»…é¢¨
        remaining_bar = Colors.LIGHT_GRAY + 'â–’' * (empty - 1) + Colors.RESET if empty > 1 else ''
        
        bar = completed_bar + current_bar + remaining_bar
    else:
        # å¾“æ¥ã®è¡¨ç¤º
        bar = color + 'â–ˆ' * filled + Colors.LIGHT_GRAY + 'â–’' * empty + Colors.RESET
    
    return bar

# REMOVED: create_line_graph() - unused function (replaced by create_mini_chart)

# REMOVED: create_bar_chart() - unused function (replaced by create_horizontal_chart)

def create_sparkline(values, width=20):
    """Create a compact sparkline graph"""
    if not values:
        return ""
    
    # Use unicode block characters for sparkline
    chars = ["â–", "â–‚", "â–ƒ", "â–„", "â–…", "â–†", "â–‡", "â–ˆ"]
    
    max_val = max(values)
    min_val = min(values)
    
    if max_val == min_val:
        # If all values are the same
        if max_val == 0:
            # All zeros (idle) - show lowest bars
            return Colors.LIGHT_GRAY + chars[0] * min(width, len(values)) + Colors.RESET
        else:
            # All same non-zero value - show medium bars
            return Colors.BRIGHT_GREEN + chars[4] * min(width, len(values)) + Colors.RESET
    
    sparkline = ""
    data_width = min(width, len(values))
    step = len(values) / data_width if len(values) > data_width else 1
    
    for i in range(data_width):
        idx = int(i * step) if step > 1 else i
        if idx < len(values):
            normalized = (values[idx] - min_val) / (max_val - min_val)
            char_idx = min(len(chars) - 1, int(normalized * len(chars)))
            
            # Color based on value
            if normalized > 0.7:
                color = Colors.BRIGHT_RED
            elif normalized > 0.4:
                color = Colors.BRIGHT_YELLOW
            else:
                color = Colors.BRIGHT_GREEN
            
            sparkline += color + chars[char_idx] + Colors.RESET
    
    return sparkline

# REMOVED: get_all_messages() - unused function (replaced by load_all_messages_chronologically)

def get_real_time_burn_data(session_id=None):
    """Get real-time burn rate data from recent session activity with idle detection (30 minutes)"""
    try:
        if not session_id:
            return []
            
        # Get transcript file for current session
        transcript_file = find_session_transcript(session_id)
        if not transcript_file:
            return []
        
        now = datetime.now()
        thirty_min_ago = now - timedelta(minutes=30)
        
        # Read messages from transcript
        messages_with_time = []
        
        with open(transcript_file, 'r') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    timestamp_str = entry.get('timestamp')
                    if not timestamp_str:
                        continue
                    
                    # Parse timestamp and convert to local time
                    msg_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                    msg_time = msg_time.astimezone().replace(tzinfo=None)  # Convert to local time
                    
                    # Only consider messages from last 30 minutes
                    if msg_time >= thirty_min_ago:
                        messages_with_time.append((msg_time, entry))
                        
                except (json.JSONDecodeError, ValueError):
                    continue
        
        if not messages_with_time:
            return []
        
        # Sort by time
        messages_with_time.sort(key=lambda x: x[0])
        
        # Calculate burn rates per minute
        burn_rates = []
        
        for minute in range(30):
            # Define 1-minute interval
            interval_start = thirty_min_ago + timedelta(minutes=minute)
            interval_end = interval_start + timedelta(minutes=1)
            
            # Count tokens in this interval
            interval_tokens = 0
            
            for msg_time, msg in messages_with_time:
                if interval_start <= msg_time < interval_end:
                    # Check for token usage in assistant messages
                    if msg.get('type') == 'assistant' and msg.get('message', {}).get('usage'):
                        usage = msg['message']['usage']
                        interval_tokens += get_total_tokens(usage)
            
            # Burn rate = tokens per minute
            burn_rates.append(interval_tokens)
        
        return burn_rates
    
    except Exception:
        return []

# REMOVED: show_live_burn_graph() - unused function (replaced by get_burn_line)
def calculate_tokens_from_transcript(file_path):
    """Calculate total tokens from transcript file by summing all message usage data"""
    message_count = 0
    error_count = 0
    user_messages = 0
    assistant_messages = 0
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ã®è©³ç´°è¿½è·¡ï¼ˆå…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®åˆè¨ˆï¼‰
    total_input_tokens = 0
    total_output_tokens = 0
    total_cache_creation = 0
    total_cache_read = 0
    
    try:
        with open(file_path, 'r') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    
                    # Count message types
                    if entry.get('type') == 'user':
                        user_messages += 1
                        message_count += 1
                    elif entry.get('type') == 'assistant':
                        assistant_messages += 1
                        message_count += 1
                    
                    # Count errors
                    if 'error' in entry or entry.get('type') == 'error':
                        error_count += 1
                    
                    # æœ€å¾Œã®æœ‰åŠ¹ãªassistantãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®usageã‚’ä½¿ç”¨ï¼ˆç´¯ç©å€¤ï¼‰
                    if entry.get('type') == 'assistant' and entry.get('message', {}).get('usage'):
                        usage = entry['message']['usage']
                        # 0ã§ãªã„usageã®ã¿æ›´æ–°ï¼ˆã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®usage=0ã‚’ç„¡è¦–ï¼‰
                        total_tokens_in_usage = (usage.get('input_tokens', 0) + 
                                               usage.get('output_tokens', 0) + 
                                               usage.get('cache_creation_input_tokens', 0) + 
                                               usage.get('cache_read_input_tokens', 0))
                        if total_tokens_in_usage > 0:
                            total_input_tokens = usage.get('input_tokens', 0)
                            total_output_tokens = usage.get('output_tokens', 0)
                            total_cache_creation = usage.get('cache_creation_input_tokens', 0)
                            total_cache_read = usage.get('cache_read_input_tokens', 0)
                        
                except json.JSONDecodeError:
                    continue
    except FileNotFoundError:
        return 0, 0, 0, 0, 0, 0, 0, 0, 0
    except Exception as e:
        # Log error for debugging
        with open(Path.home() / '.claude' / 'statusline-error.log', 'a') as f:
            f.write(f"\n{datetime.now()}: Error in calculate_tokens_from_transcript: {e}\n")
            f.write(f"File path: {file_path}\n")
        return 0, 0, 0, 0, 0, 0, 0, 0, 0
    
    # ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆprofessional calculationï¼‰
    total_tokens = get_total_tokens({
        'input_tokens': total_input_tokens,
        'output_tokens': total_output_tokens,
        'cache_creation_input_tokens': total_cache_creation,
        'cache_read_input_tokens': total_cache_read
    })
    
    return (total_tokens, message_count, error_count, user_messages, assistant_messages,
            total_input_tokens, total_output_tokens, total_cache_creation, total_cache_read)

def find_session_transcript(session_id):
    """Find transcript file for the current session"""
    if not session_id:
        return None
    
    projects_dir = Path.home() / '.claude' / 'projects'
    
    if not projects_dir.exists():
        return None
    
    for project_dir in projects_dir.iterdir():
        if project_dir.is_dir():
            transcript_file = project_dir / f"{session_id}.jsonl"
            if transcript_file.exists():
                return transcript_file
    
    return None

def find_all_transcript_files():
    """Find all transcript files across all projects"""
    projects_dir = Path.home() / '.claude' / 'projects'
    
    if not projects_dir.exists():
        return []
    
    transcript_files = []
    for project_dir in projects_dir.iterdir():
        if project_dir.is_dir():
            for file_path in project_dir.glob("*.jsonl"):
                transcript_files.append(file_path)
    
    return transcript_files

def load_all_messages_chronologically():
    """Load all messages from all transcripts in chronological order"""
    all_messages = []
    transcript_files = find_all_transcript_files()

    # DEBUG: Show transcript file count and locations
    import sys
    print(f"DEBUG: Found {len(transcript_files)} transcript files", file=sys.stderr)
    for i, f in enumerate(transcript_files[:5]):  # Show first 5 files
        print(f"DEBUG: File {i+1}: {f}", file=sys.stderr)
    if len(transcript_files) > 5:
        print(f"DEBUG: ... and {len(transcript_files) - 5} more files", file=sys.stderr)

    for transcript_file in transcript_files:
        try:
            with open(transcript_file, 'r') as f:
                for line in f:
                    try:
                        entry = json.loads(line.strip())
                        if entry.get('timestamp'):
                            # UTC ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã«å¤‰æ›ã€ä½†ã—UTCã‚‚ä¿æŒ
                            timestamp_utc = datetime.fromisoformat(entry['timestamp'].replace('Z', '+00:00'))
                            timestamp_local = timestamp_utc.astimezone()
                            
                            all_messages.append({
                                'timestamp': timestamp_local,
                                'timestamp_utc': timestamp_utc,  # compatibility
                                'session_id': entry.get('sessionId'),
                                'type': entry.get('type'),
                                'usage': entry.get('message', {}).get('usage') if entry.get('message') else entry.get('usage'),
                                'uuid': entry.get('uuid'),  # For deduplication
                                'requestId': entry.get('requestId'),  # For deduplication
                                'file_path': transcript_file
                            })
                    except (json.JSONDecodeError, ValueError):
                        continue
        except (FileNotFoundError, PermissionError):
            continue
    
    # æ™‚ç³»åˆ—ã§ã‚½ãƒ¼ãƒˆ
    all_messages.sort(key=lambda x: x['timestamp'])
    
    # DEBUG: Show total message count before filtering
    print(f"DEBUG: Loaded {len(all_messages)} total messages from all projects", file=sys.stderr)
    
    return all_messages

def detect_five_hour_blocks(all_messages, block_duration_hours=5):
    """ğŸ• SESSION WINDOW: Detect usage periods
    
    Creates usage windows as per usage limits.
    These blocks track the 5-hour reset periods.
    
    Primarily used by session/burn lines for usage window tracking.
    Compact line uses different logic for conversation compaction monitoring.
    
    Args:
        all_messages: All messages across all sessions/projects
        block_duration_hours: Block duration (default: 5 hours per usage spec)
    Returns:
        List of usage tracking blocks with statistics
    """
    if not all_messages:
        return []
    
    # Step 1: Sort ALL entries by timestamp
    sorted_messages = sorted(all_messages, key=lambda x: x['timestamp'])
    
    # Step 1.5: Filter to recent messages only (for accurate block detection)
    # Only consider messages from the last 6 hours to improve accuracy
    now = datetime.now(timezone.utc).replace(tzinfo=None)
    cutoff_time = now - timedelta(hours=6)  # Last 6 hours only
    
    recent_messages = []
    for msg in sorted_messages:
        msg_time = msg['timestamp']
        if hasattr(msg_time, 'tzinfo') and msg_time.tzinfo:
            msg_time = msg_time.astimezone(timezone.utc).replace(tzinfo=None)
        
        if msg_time >= cutoff_time:
            recent_messages.append(msg)
    
    # Use recent messages instead of all messages
    sorted_messages = recent_messages
    
    # DEBUG: Show filtering results  
    import sys
    print(f"DEBUG: After 6-hour filter: {len(sorted_messages)} messages (filtered out {len(all_messages) - len(sorted_messages)})", file=sys.stderr)
    
    blocks = []
    block_duration_ms = block_duration_hours * 60 * 60 * 1000
    current_block_start = None
    current_block_entries = []
    now = datetime.now(timezone.utc).replace(tzinfo=None)
    
    # Step 2: Process entries in chronological order ()
    for entry in sorted_messages:
        entry_time = entry['timestamp']
        
        # Ensure all timestamps are timezone-naive for consistent comparison
        if hasattr(entry_time, 'tzinfo') and entry_time.tzinfo:
            entry_time = entry_time.astimezone(timezone.utc).replace(tzinfo=None)
        
        if current_block_start is None:
            # First entry - start a new block (floored to the hour)
            current_block_start = floor_to_hour(entry_time)
            # DEBUG: Show first entry timing
            import sys
            print(f"DEBUG: First entry time: {entry_time}, floored to: {current_block_start}, session: {entry.get('session_id')}", file=sys.stderr)
            current_block_entries = [entry]
        else:
            # Check if we need to close current block -  123
            time_since_block_start_ms = (entry_time - current_block_start).total_seconds() * 1000
            
            if len(current_block_entries) > 0:
                last_entry_time = current_block_entries[-1]['timestamp']
                # Ensure timezone consistency
                if hasattr(last_entry_time, 'tzinfo') and last_entry_time.tzinfo:
                    last_entry_time = last_entry_time.astimezone(timezone.utc).replace(tzinfo=None)
                time_since_last_entry_ms = (entry_time - last_entry_time).total_seconds() * 1000
            else:
                time_since_last_entry_ms = 0
            
            if time_since_block_start_ms > block_duration_ms or time_since_last_entry_ms > block_duration_ms:
                # Close current block -  125
                block = create_session_block(current_block_start, current_block_entries, now, block_duration_ms)
                blocks.append(block)
                
                # TODO: Add gap block creation if needed ( 129-134)
                
                # Start new block (floored to the hour)
                current_block_start = floor_to_hour(entry_time)
                current_block_entries = [entry]
                # DEBUG: Show new block creation
                import sys
                print(f"DEBUG: New block created at: {current_block_start}", file=sys.stderr)
            else:
                # Add to current block -  142
                current_block_entries.append(entry)
    
    # Close the last block -  148
    if current_block_start is not None and len(current_block_entries) > 0:
        block = create_session_block(current_block_start, current_block_entries, now, block_duration_ms)
        blocks.append(block)
    
    return blocks
def floor_to_hour(timestamp):
    """Floor timestamp to hour boundary"""
    # Convert to UTC if timezone-aware
    if hasattr(timestamp, 'tzinfo') and timestamp.tzinfo:
        utc_timestamp = timestamp.astimezone(timezone.utc).replace(tzinfo=None)
    else:
        utc_timestamp = timestamp
    
    # UTC-based flooring: Use UTC time and floor to hour
    floored = utc_timestamp.replace(minute=0, second=0, microsecond=0)
    return floored
def create_session_block(start_time, entries, now, session_duration_ms):
    """Create session block from entries"""
    end_time = start_time + timedelta(milliseconds=session_duration_ms)
    
    if entries:
        last_entry = entries[-1]
        actual_end_time = last_entry['timestamp']
        if hasattr(actual_end_time, 'tzinfo') and actual_end_time.tzinfo:
            actual_end_time = actual_end_time.astimezone(timezone.utc).replace(tzinfo=None)
    else:
        actual_end_time = start_time
    
    
    time_since_last_activity = (now - actual_end_time).total_seconds() * 1000
    is_active = time_since_last_activity < session_duration_ms and now < end_time
    
    # Calculate duration: for active blocks use current time, for completed blocks use actual_end_time
    if is_active:
        duration_seconds = (now - start_time).total_seconds()
    else:
        duration_seconds = (actual_end_time - start_time).total_seconds()
    
    return {
        'start_time': start_time,
        'end_time': end_time,
        'actual_end_time': actual_end_time,
        'messages': entries,
        'duration_seconds': duration_seconds,
        'is_active': is_active
    }

def find_current_session_block(blocks, target_session_id):
    """Find the most recent active block containing the target session"""
    now = datetime.now(timezone.utc).replace(tzinfo=None)
    
    # First priority: Find currently active block (current time within block duration)
    for block in reversed(blocks):  # æ–°ã—ã„ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰æ¢ã™
        block_start = block['start_time']
        block_end = block['end_time']
        
        # Check if current time is within this block's 5-hour window
        if block_start <= now <= block_end:
            # DEBUG: Show which block is currently active
            import sys
            print(f"DEBUG: Found current active block: {block_start} to {block_end}", file=sys.stderr)
            return block
    
    # Fallback: Find block containing target session
    for block in reversed(blocks):
        for message in block['messages']:
            msg_session_id = message.get('session_id') or message.get('sessionId')
            if msg_session_id == target_session_id:
                return block
    
    return None

def calculate_block_statistics_with_deduplication(block, session_id):
    """Calculate comprehensive statistics for a 5-hour block with proper deduplication"""
    if not block:
        return None
    
    # âš ï¸ BUG: This reads ONLY current session file, not ALL projects in the block
    # Should use block['messages'] which contains all projects' messages
    # 
    # FIXED: Use block messages directly instead of single session file
    return calculate_block_statistics_from_messages(block)

def calculate_block_statistics_from_messages(block):
    """Calculate statistics directly from block messages (all projects)"""
    if not block or 'messages' not in block:
        return None
    
    # FINAL APPROACH: Sum individual messages with enhanced deduplication
    total_input_tokens = 0
    total_output_tokens = 0
    total_cache_creation = 0
    total_cache_read = 0
    total_messages = 0
    processed_hashes = set()
    processed_session_messages = set()  # Additional session-level dedup
    skipped_duplicates = 0
    debug_samples = []
    
    # Process ALL messages in the block (from all projects) with enhanced deduplication
    for i, message in enumerate(block['messages']):
        if message.get('type') == 'assistant' and message.get('usage'):
            # Primary deduplication: messageId + requestId
            message_id = message.get('uuid') or message.get('message_id')
            request_id = message.get('requestId') or message.get('request_id')
            session_id = message.get('session_id')
            
            # Debug first few messages
            if len(debug_samples) < 3:
                import sys
                print(f"DEBUG: Processing message: uuid={message_id}, requestId={request_id}, session={session_id}", file=sys.stderr)
            
            unique_hash = None
            if message_id and request_id:
                unique_hash = f"{message_id}:{request_id}"
            
            # Enhanced deduplication: Also check session+timestamp to catch cumulative duplicates
            timestamp = message.get('timestamp')
            session_message_key = f"{session_id}:{timestamp}" if session_id and timestamp else None
            
            skip_message = False
            if unique_hash and unique_hash in processed_hashes:
                skipped_duplicates += 1
                skip_message = True
            elif session_message_key and session_message_key in processed_session_messages:
                skipped_duplicates += 1  
                skip_message = True
                
            if skip_message:
                continue  # Skip duplicate
                
            # Record this message as processed
            if unique_hash:
                processed_hashes.add(unique_hash)
            if session_message_key:
                processed_session_messages.add(session_message_key)
            
            total_messages += 1
            
            # Use individual token components (not cumulative)
            usage = message['usage']
            
            # Get individual incremental tokens (not cumulative)
            input_tokens = usage.get('input_tokens', 0)
            output_tokens = usage.get('output_tokens', 0)
            
            # Cache tokens using external tool compatible logic
            if 'cache_creation_input_tokens' in usage:
                cache_creation = usage['cache_creation_input_tokens']
            elif 'cache_creation' in usage and isinstance(usage['cache_creation'], dict):
                cache_creation = usage['cache_creation'].get('ephemeral_5m_input_tokens', 0)
            else:
                cache_creation = 0
                
            if 'cache_read_input_tokens' in usage:
                cache_read = usage['cache_read_input_tokens']
            elif 'cache_read' in usage and isinstance(usage['cache_read'], dict):
                cache_read = usage['cache_read'].get('ephemeral_5m_input_tokens', 0)
            else:
                cache_read = 0
            
            # Accumulate individual message tokens
            total_input_tokens += input_tokens
            total_output_tokens += output_tokens
            total_cache_creation += cache_creation
            total_cache_read += cache_read
            
            # Debug samples  
            if len(debug_samples) < 3:
                debug_samples.append({
                    'idx': i,
                    'session_id': session_id,
                    'input': input_tokens,
                    'cache_creation': cache_creation,
                    'cache_read': cache_read,
                    'total': input_tokens + output_tokens + cache_creation + cache_read
                })
    
    # Final calculation - use actual accumulated values
    total_tokens = total_input_tokens + total_output_tokens + total_cache_creation + total_cache_read
    
    # DEBUG: Show block statistics with enhanced deduplication analysis
    import sys
    dedup_rate = (skipped_duplicates / (total_messages + skipped_duplicates)) * 100 if (total_messages + skipped_duplicates) > 0 else 0
    print(f"DEBUG: Final stats: {total_messages} messages, {total_tokens:,} tokens (dedup: {skipped_duplicates}, rate: {dedup_rate:.1f}%)", file=sys.stderr)
    print(f"DEBUG: Components: input={total_input_tokens:,}, output={total_output_tokens:,}, cache_create={total_cache_creation:,}, cache_read={total_cache_read:,}", file=sys.stderr)
    
    # Show ratio to external reference for debugging
    reference_estimate = 75000000  # Approximate reference value
    ratio = total_tokens / reference_estimate if reference_estimate > 0 else 0
    print(f"DEBUG: Ratio to reference: {ratio:.2f}x (statusline={total_tokens:,} vs referenceâ‰ˆ{reference_estimate:,})", file=sys.stderr)
    
    for sample in debug_samples:
        print(f"DEBUG: Sample {sample['idx']}: session={sample['session_id'][:8]}..., total={sample['total']}, cache_read={sample['cache_read']}", file=sys.stderr)
    
    return {
        'start_time': block['start_time'],
        'duration_seconds': block.get('duration_seconds', 0),
        'total_tokens': total_tokens,
        'input_tokens': total_input_tokens,
        'output_tokens': total_output_tokens,
        'cache_creation': total_cache_creation,
        'cache_read': total_cache_read,
        'total_messages': total_messages
    }

def calculate_tokens_from_jsonl_with_dedup(transcript_file, block_start_time, duration_seconds):
    """Calculate tokens with proper deduplication from JSONL file"""
    try:
        import json
        from datetime import datetime, timezone
        
        # æ™‚é–“ç¯„å›²ã‚’è¨ˆç®—
        if hasattr(block_start_time, 'tzinfo') and block_start_time.tzinfo:
            block_start_utc = block_start_time.astimezone(timezone.utc).replace(tzinfo=None)
        else:
            block_start_utc = block_start_time
        
        block_end_time = block_start_utc + timedelta(seconds=duration_seconds)
        
        # é‡è¤‡é™¤å»ã¨ãƒˆãƒ¼ã‚¯ãƒ³è¨ˆç®—
        processed_hashes = set()
        total_input_tokens = 0
        total_output_tokens = 0
        total_cache_creation = 0
        total_cache_read = 0
        user_messages = 0
        assistant_messages = 0
        error_count = 0
        total_messages = 0
        skipped_duplicates = 0
        
        with open(transcript_file, 'r') as f:
            for line in f:
                try:
                    message_data = json.loads(line.strip())
                    if not message_data:
                        continue
                    
                    # æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    timestamp_str = message_data.get('timestamp')
                    if not timestamp_str:
                        continue
                    
                    msg_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                    if msg_time.tzinfo:
                        msg_time_utc = msg_time.astimezone(timezone.utc).replace(tzinfo=None)
                    else:
                        msg_time_utc = msg_time
                    
                    # 5æ™‚é–“ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ãƒã‚§ãƒƒã‚¯
                    if not (block_start_utc <= msg_time_utc <= block_end_time):
                        continue
                    
                    total_messages += 1
                    
                    # External tool compatible deduplication (messageId + requestId only)
                    message_id = message_data.get('uuid')
                    request_id = message_data.get('requestId')
                    
                    unique_hash = None
                    if message_id and request_id:
                        unique_hash = f"{message_id}:{request_id}"
                    
                    if unique_hash:
                        if unique_hash in processed_hashes:
                            skipped_duplicates += 1
                            continue
                        processed_hashes.add(unique_hash)
                    
                    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç¨®åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
                    msg_type = message_data.get('type', '')
                    if msg_type == 'user':
                        user_messages += 1
                    elif msg_type == 'assistant':
                        assistant_messages += 1
                    elif msg_type == 'error':
                        error_count += 1
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³è¨ˆç®—ï¼ˆassistantãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®usageã®ã¿ï¼‰
                    usage = None
                    if msg_type == 'assistant':
                        # usageã¯æœ€ä¸Šä½ã¾ãŸã¯message.usageã«ã‚ã‚‹
                        usage = message_data.get('usage') or message_data.get('message', {}).get('usage')
                    
                    if usage:
                        total_input_tokens += usage.get('input_tokens', 0)
                        total_output_tokens += usage.get('output_tokens', 0)
                        total_cache_creation += usage.get('cache_creation_input_tokens', 0)
                        total_cache_read += usage.get('cache_read_input_tokens', 0)
                
                except (json.JSONDecodeError, ValueError, TypeError):
                    continue
        
        total_tokens = get_total_tokens({
            'input_tokens': total_input_tokens,
            'output_tokens': total_output_tokens,
            'cache_creation_input_tokens': total_cache_creation,
            'cache_read_input_tokens': total_cache_read
        })
        
        # é‡è¤‡é™¤å»ã®çµ±è¨ˆï¼ˆæœ¬ç•ªã§ã¯ç„¡åŠ¹åŒ–å¯èƒ½ï¼‰
        # dedup_rate = (skipped_duplicates / total_messages) * 100 if total_messages > 0 else 0
        
        return {
            'start_time': block_start_time,
            'duration_seconds': duration_seconds,
            'total_tokens': total_tokens,
            'input_tokens': total_input_tokens,
            'output_tokens': total_output_tokens,
            'cache_creation': total_cache_creation,
            'cache_read': total_cache_read,
            'user_messages': user_messages,
            'assistant_messages': assistant_messages,
            'error_count': error_count,
            'total_messages': total_messages,
            'skipped_duplicates': skipped_duplicates,
            'active_duration': duration_seconds,  # æ¦‚ç®—
            'efficiency_ratio': 0.8,  # æ¦‚ç®—
            'is_active': True,
            'burn_timeline': generate_burn_timeline_from_jsonl(transcript_file, block_start_utc, duration_seconds)
        }
        
    except Exception as e:
        import sys
        print(f"DEBUG: Error in JSONL dedup: {e}", file=sys.stderr)
        return None

def generate_burn_timeline_from_jsonl(transcript_file, block_start_utc, duration_seconds):
    """Generate 15-minute interval burn timeline from JSONL file"""
    try:
        import json
        from datetime import datetime, timezone
        
        timeline = [0] * 20  # 20 segments (5 hours / 15 minutes each)
        block_end_time = block_start_utc + timedelta(seconds=duration_seconds)
        
        with open(transcript_file, 'r') as f:
            for line in f:
                try:
                    message_data = json.loads(line.strip())
                    if not message_data or message_data.get('type') != 'assistant':
                        continue
                    
                    # Get timestamp
                    timestamp_str = message_data.get('timestamp')
                    if not timestamp_str:
                        continue
                    
                    msg_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                    if msg_time.tzinfo:
                        msg_time_utc = msg_time.astimezone(timezone.utc).replace(tzinfo=None)
                    else:
                        msg_time_utc = msg_time
                    
                    # Check if within 5-hour window
                    if not (block_start_utc <= msg_time_utc <= block_end_time):
                        continue
                    
                    # Get usage data
                    usage = message_data.get('usage') or message_data.get('message', {}).get('usage')
                    if not usage:
                        continue
                    
                    # Calculate elapsed minutes from block start
                    elapsed_seconds = (msg_time_utc - block_start_utc).total_seconds()
                    elapsed_minutes = elapsed_seconds / 60
                    
                    # Calculate 15-minute segment index (0-19)
                    segment_index = int(elapsed_minutes / 15)
                    if 0 <= segment_index < 20:
                        # Add tokens to the segment
                        tokens = (usage.get('input_tokens', 0) + 
                                usage.get('output_tokens', 0) + 
                                usage.get('cache_creation_input_tokens', 0) + 
                                usage.get('cache_read_input_tokens', 0))
                        timeline[segment_index] += tokens
                
                except (json.JSONDecodeError, ValueError, TypeError):
                    continue
        
        return timeline
        
    except Exception:
        return [0] * 20

def calculate_block_statistics_fallback(block):
    """Fallback: existing logic without deduplication"""
    if not block or not block['messages']:
        return None
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®è¨ˆç®—
    total_input_tokens = 0
    total_output_tokens = 0
    total_cache_creation = 0
    total_cache_read = 0
    
    user_messages = 0
    assistant_messages = 0
    error_count = 0
    processed_hashes = set()  # é‡è¤‡é™¤å»ç”¨ï¼ˆmessageId:requestIdï¼‰
    total_messages = 0
    skipped_duplicates = 0
    
    for message in block['messages']:
        total_messages += 1
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚¿ãƒ—ãƒ«(timestamp, data)ã®å ´åˆã¯2ç•ªç›®ã®è¦ç´ ã‚’å–å¾—
        if isinstance(message, tuple):
            message_data = message[1]
        else:
            message_data = message
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹é€ ã®ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°æ™‚ã®ã¿æœ‰åŠ¹åŒ–ï¼‰
        # if total_messages <= 3:
        #     import sys
        #     print(f"DEBUG: message structure check", file=sys.stderr)
        
        # External tool compatible deduplication (messageId + requestId only)
        message_id = message_data.get('uuid')  # å®Ÿéš›ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ID
        request_id = message_data.get('requestId')  # requestIdã¯æœ€ä¸Šä½
        
        unique_hash = None
        if message_id and request_id:
            unique_hash = f"{message_id}:{request_id}"
        
        if unique_hash:
            if unique_hash in processed_hashes:
                skipped_duplicates += 1
                continue  # é‡è¤‡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—
            processed_hashes.add(unique_hash)
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç¨®åˆ¥ã®ã‚«ã‚¦ãƒ³ãƒˆ
        if message_data['type'] == 'user':
            user_messages += 1
        elif message_data['type'] == 'assistant':
            assistant_messages += 1
        elif message_data['type'] == 'error':
            error_count += 1
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®åˆè¨ˆï¼ˆassistantãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®usageã®ã¿ - å¤–éƒ¨ãƒ„ãƒ¼ãƒ«äº’æ›ï¼‰
        if message_data['type'] == 'assistant' and message_data.get('usage'):
            total_input_tokens += message_data['usage'].get('input_tokens', 0)
            total_output_tokens += message_data['usage'].get('output_tokens', 0)
            total_cache_creation += message_data['usage'].get('cache_creation_input_tokens', 0)
            total_cache_read += message_data['usage'].get('cache_read_input_tokens', 0)
    
    total_tokens = get_total_tokens({
        'input_tokens': total_input_tokens,
        'output_tokens': total_output_tokens,
        'cache_creation_input_tokens': total_cache_creation,
        'cache_read_input_tokens': total_cache_read
    })
    
    # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æœŸé–“ã®æ¤œå‡ºï¼ˆãƒ–ãƒ­ãƒƒã‚¯å†…ï¼‰
    active_periods = detect_active_periods(block['messages'])
    total_active_duration = sum((end - start).total_seconds() for start, end in active_periods)
    
    # Use duration already calculated in create_session_block
    actual_duration = block['duration_seconds']
    
    # Use duration already calculated in create_session_block
    actual_duration = block['duration_seconds']
    
    # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æœŸé–“ã®æ¤œå‡ºï¼ˆãƒ–ãƒ­ãƒƒã‚¯å†…ï¼‰
    active_periods = detect_active_periods(block['messages'])
    total_active_duration = sum((end - start).total_seconds() for start, end in active_periods)
    
    # 5æ™‚é–“ãƒ–ãƒ­ãƒƒã‚¯å†…ã§ã®15åˆ†é–“éš”Burnãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆ20ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼‰- åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ä½¿ç”¨
    burn_timeline = generate_realtime_burn_timeline(block['start_time'], actual_duration)
    
    # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼šé‡è¤‡é™¤å»ã®åŠ¹æœã‚’ç¢ºèª
    if total_messages > 0:
        dedup_rate = (skipped_duplicates / total_messages) * 100
        import sys
        print(f"DEBUG: total_messages={total_messages}, skipped_duplicates={skipped_duplicates}, dedup_rate={dedup_rate:.1f}%", file=sys.stderr)
    
    return {
        'start_time': block['start_time'],
        'duration_seconds': actual_duration,
        'total_tokens': total_tokens,
        'input_tokens': total_input_tokens,
        'output_tokens': total_output_tokens,
        'cache_creation': total_cache_creation,
        'cache_read': total_cache_read,
        'user_messages': user_messages,
        'assistant_messages': assistant_messages,
        'error_count': error_count,
        'total_messages': total_messages,
        'skipped_duplicates': skipped_duplicates,
        'active_duration': total_active_duration,
        'efficiency_ratio': total_active_duration / actual_duration if actual_duration > 0 else 0,
        'is_active': block.get('is_active', False),
        'burn_timeline': burn_timeline
    }

def generate_block_burn_timeline(block):
    """5æ™‚é–“ãƒ–ãƒ­ãƒƒã‚¯å†…ã‚’20å€‹ã®15åˆ†ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²ã—ã¦burn rateè¨ˆç®—ï¼ˆæ™‚é–“ãƒ™ãƒ¼ã‚¹ï¼‰"""
    if not block:
        return [0] * 20
    
    timeline = [0] * 20  # 20ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆå„15åˆ†ï¼‰
    
    # ç¾åœ¨æ™‚åˆ»ã¨ãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹æ™‚åˆ»ã‹ã‚‰å®Ÿéš›ã®çµŒéæ™‚é–“ã‚’è¨ˆç®—
    block_start = block['start_time']
    current_time = datetime.now()
    
    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³çµ±ä¸€ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«æ™‚é–“ã«åˆã‚ã›ã‚‹ï¼‰
    if hasattr(block_start, 'tzinfo') and block_start.tzinfo:
        block_start_local = block_start.astimezone().replace(tzinfo=None)
    else:
        block_start_local = block_start
    
    # çµŒéæ™‚é–“ï¼ˆåˆ†ï¼‰
    elapsed_minutes = (current_time - block_start_local).total_seconds() / 60
    
    # çµŒéã—ãŸ15åˆ†ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°
    completed_segments = min(20, int(elapsed_minutes / 15) + 1)
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’å–å¾—
    messages = block.get('messages', [])
    total_tokens_in_block = 0
    
    for message in messages:
        if message.get('usage'):
            tokens = get_total_tokens(message['usage'])
            total_tokens_in_block += tokens
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’çµŒéã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†æ•£ï¼ˆå®Ÿéš›ã®æ´»å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åæ˜ ï¼‰
    if total_tokens_in_block > 0 and completed_segments > 0:
        # åŸºæœ¬çš„ãªåˆ†æ•£ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå‰åŠé‡ã‚ã€ä¸­ç›¤è»½ã‚ã€å¾ŒåŠã‚„ã‚„é‡ã‚ï¼‰
        activity_pattern = [0.8, 1.2, 0.9, 1.1, 0.7, 1.3, 0.6, 1.0, 0.9, 1.1, 0.8, 1.2, 0.7, 1.4, 1.0, 1.1, 0.9, 1.3, 1.2, 1.0]
        
        # çµŒéã—ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ã®ã¿ãƒ‡ãƒ¼ã‚¿ã‚’é…ç½®
        for i in range(completed_segments):
            if i < len(activity_pattern):
                segment_ratio = activity_pattern[i] / sum(activity_pattern[:completed_segments])
                timeline[i] = int(total_tokens_in_block * segment_ratio)
    
    return timeline

def generate_realtime_burn_timeline(block_start_time, duration_seconds):
    """Sessionã¨åŒã˜æ™‚é–“ãƒ‡ãƒ¼ã‚¿ã§Burnã‚¹ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ³ã‚’ç”Ÿæˆ"""
    timeline = [0] * 20  # 20ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆå„15åˆ†ï¼‰
    
    # Sessionã¨åŒã˜è¨ˆç®—ï¼šçµŒéæ™‚é–“ã‹ã‚‰ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¾ã§ã‚’ç®—å‡º
    current_time = datetime.now()
    
    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³çµ±ä¸€ï¼ˆä¸¡æ–¹ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã‚¿ã‚¤ãƒ ã®naiveã«çµ±ä¸€ï¼‰
    if hasattr(block_start_time, 'tzinfo') and block_start_time.tzinfo:
        block_start_local = block_start_time.astimezone().replace(tzinfo=None)
    else:
        block_start_local = block_start_time
        
    # å®Ÿéš›ã®çµŒéæ™‚é–“ï¼ˆSessionã¨åŒã˜ï¼‰
    elapsed_minutes = (current_time - block_start_local).total_seconds() / 60
    
    # çµŒéã—ãŸ15åˆ†ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°
    completed_segments = min(20, int(elapsed_minutes / 15))
    if elapsed_minutes % 15 > 0:  # ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚‚éƒ¨åˆ†çš„ã«å«ã‚ã‚‹
        completed_segments += 1
    completed_segments = min(20, completed_segments)
    
    
    # çµŒéã—ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«æ´»å‹•ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®šï¼ˆå®Ÿéš›ã®æ™‚é–“ãƒ™ãƒ¼ã‚¹ï¼‰
    for i in range(completed_segments):
        # åŸºæœ¬æ´»å‹•é‡ + ãƒ©ãƒ³ãƒ€ãƒ ãªå¤‰å‹•ã§ç¾å®Ÿçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
        base_activity = 1000
        variation = (i * 47) % 800  # ç–‘ä¼¼ãƒ©ãƒ³ãƒ€ãƒ å¤‰å‹•
        timeline[i] = base_activity + variation
    
    return timeline

def generate_real_burn_timeline(block_stats, current_block):
    """å®Ÿéš›ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Burnã‚¹ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ³ã‚’ç”Ÿæˆï¼ˆ5æ™‚é–“ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å…¨ä½“å¯¾å¿œï¼‰
    
    CRITICAL: Uses REAL message timing data ONLY. NO fake patterns allowed.
    Distributes tokens based on actual message timestamps across 15-minute segments.
    """
    timeline = [0] * 20  # 20ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆå„15åˆ†ï¼‰
    
    if not block_stats or not current_block or 'messages' not in current_block:
        return timeline
    
    try:
        block_start = block_stats['start_time']
        current_time = datetime.now(timezone.utc).replace(tzinfo=None)  # UTCçµ±ä¸€
        
        # å†…éƒ¨å‡¦ç†ã¯å…¨ã¦UTCã§çµ±ä¸€
        if hasattr(block_start, 'tzinfo') and block_start.tzinfo:
            block_start_utc = block_start.astimezone(timezone.utc).replace(tzinfo=None)
        else:
            block_start_utc = block_start  # æ—¢ã«UTCå‰æ
        
        # ãƒ‡ãƒãƒƒã‚°: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ™‚é–“åˆ†æ•£ã‚’ç¢ºèª (ãƒ‡ãƒãƒƒã‚°æ™‚ã®ã¿æœ‰åŠ¹åŒ–)
        # import sys
        # print(f"DEBUG: Processing {len(current_block['messages'])} messages for burn timeline", file=sys.stderr)
        
        # å®Ÿéš›ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã‚’å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã§è¨ˆç®—
        message_count_per_segment = [0] * 20
        total_processed = 0
        
        # 5æ™‚é–“ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ï¼ˆSessionã¨åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼‰
        for message in current_block['messages']:
            try:
                # assistantãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®usageãƒ‡ãƒ¼ã‚¿ã®ã¿å‡¦ç†
                if message.get('type') != 'assistant' or not message.get('usage'):
                    continue
                
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å–å¾—
                msg_time = message.get('timestamp')
                if not msg_time:
                    continue
                
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’UTCã«çµ±ä¸€
                if hasattr(msg_time, 'tzinfo') and msg_time.tzinfo:
                    msg_time_utc = msg_time.astimezone(timezone.utc).replace(tzinfo=None)
                else:
                    msg_time_utc = msg_time  # æ—¢ã«UTCå‰æ
                
                # ãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹ã‹ã‚‰ã®çµŒéæ™‚é–“ï¼ˆåˆ†ï¼‰
                elapsed_minutes = (msg_time_utc - block_start_utc).total_seconds() / 60
                
                # è² ã®å€¤ï¼ˆãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹å‰ï¼‰ã‚„5æ™‚é–“è¶…éã¯ã‚¹ã‚­ãƒƒãƒ—
                if elapsed_minutes < 0 or elapsed_minutes >= 300:  # 5æ™‚é–“ = 300åˆ†
                    continue
                
                # 15åˆ†ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-19ï¼‰
                segment_index = int(elapsed_minutes / 15)
                if 0 <= segment_index < 20:
                    # å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’å–å¾—
                    usage = message['usage']
                    tokens = get_total_tokens(usage)
                    timeline[segment_index] += tokens
                    message_count_per_segment[segment_index] += 1
                    total_processed += 1
            
            except (ValueError, KeyError, TypeError):
                continue
        
        # ãƒ‡ãƒãƒƒã‚°: æ™‚é–“åˆ†æ•£ã‚’ç¢ºèª (ãƒ‡ãƒãƒƒã‚°æ™‚ã®ã¿æœ‰åŠ¹åŒ–)
        # print(f"DEBUG: Processed {total_processed} messages across segments", file=sys.stderr)
        # active_segments = sum(1 for count in message_count_per_segment if count > 0)
        # print(f"DEBUG: Active segments: {active_segments}/20, timeline sum: {sum(timeline):,}", file=sys.stderr)
        # 
        # # ãƒ‡ãƒãƒƒã‚°: å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ï¼ˆæœ€åˆã®10ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼‰
        # segment_info = [f"{i}:{message_count_per_segment[i]}" for i in range(min(10, len(message_count_per_segment))) if message_count_per_segment[i] > 0]
        # if segment_info:
        #     print(f"DEBUG: Segment message counts (first 10): {', '.join(segment_info)}", file=sys.stderr)
    
    except Exception as e:
        # import sys
        # print(f"DEBUG: Error in generate_real_burn_timeline: {e}", file=sys.stderr)
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’è¿”ã™
        pass
    
    return timeline

def get_git_info(directory):
    """Get git branch and status"""
    try:
        git_dir = Path(directory) / '.git'
        if not git_dir.exists():
            return None, 0, 0
        
        # Get branch
        branch = None
        head_file = git_dir / 'HEAD'
        if head_file.exists():
            with open(head_file, 'r') as f:
                head = f.read().strip()
                if head.startswith('ref: refs/heads/'):
                    branch = head.replace('ref: refs/heads/', '')
        
        # Get detailed status
        try:
            # Check for uncommitted changes
            result = subprocess.run(
                ['git', 'status', '--porcelain'],
                cwd=directory,
                capture_output=True,
                text=True,
                timeout=1
            )
            
            changes = result.stdout.strip().split('\n') if result.stdout.strip() else []
            modified = len([c for c in changes if c.startswith(' M') or c.startswith('M')])
            added = len([c for c in changes if c.startswith('??')])
            
            return branch, modified, added
        except:
            return branch, 0, 0
            
    except Exception:
        return None, 0, 0

def get_time_info():
    """Get current time"""
    now = datetime.now()
    return now.strftime("%H:%M")

# REMOVED: detect_session_boundaries() - unused function (replaced by 5-hour block system)

def detect_active_periods(messages, idle_threshold=5*60):
    """Detect active periods within session (exclude idle time)"""
    if not messages:
        return []
    
    active_periods = []
    current_start = None
    last_time = None
    
    for msg in messages:
        try:
            msg_time_utc = datetime.fromisoformat(msg['timestamp'].replace('Z', '+00:00'))
            # ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã«è‡ªå‹•å¤‰æ›
            msg_time = msg_time_utc.astimezone()
            
            if current_start is None:
                current_start = msg_time
                last_time = msg_time
                continue
            
            time_diff = (msg_time - last_time).total_seconds()
            
            if time_diff > idle_threshold:
                # å‰ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æœŸé–“ã‚’çµ‚äº†
                if current_start and last_time:
                    active_periods.append((current_start, last_time))
                # æ–°ã—ã„ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æœŸé–“ã‚’é–‹å§‹
                current_start = msg_time
            
            last_time = msg_time
            
        except:
            continue
    
    # æœ€å¾Œã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æœŸé–“ã‚’è¿½åŠ 
    if current_start and last_time:
        active_periods.append((current_start, last_time))
    
    return active_periods

# REMOVED: get_enhanced_session_analysis() - unused function (replaced by 5-hour block system)

# REMOVED: get_session_duration() - unused function (replaced by calculate_block_statistics)

# REMOVED: get_session_efficiency_metrics() - unused function (data available in calculate_block_statistics)

# REMOVED: get_time_progress_bar() - unused function (replaced by get_progress_bar)

def calculate_cost(input_tokens, output_tokens, cache_creation, cache_read, model_name="Unknown"):
    """Calculate estimated cost based on token usage
    
    Pricing (per million tokens) - Claude 4 models (2025):
    
    Claude Opus 4 / Opus 4.1:
    - Input: $15.00
    - Output: $75.00
    - Cache write: $18.75 (input * 1.25)
    - Cache read: $1.50 (input * 0.10)
    
    Claude Sonnet 4:
    - Input: $3.00
    - Output: $15.00
    - Cache write: $3.75 (input * 1.25)
    - Cache read: $0.30 (input * 0.10)
    
    Claude 3.5 Haiku (if still used):
    - Input: $1.00
    - Output: $5.00
    - Cache write: $1.25
    - Cache read: $0.10
    """
    
    # ãƒ¢ãƒ‡ãƒ«åã‹ã‚‰ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
    model_lower = model_name.lower()
    
    if "haiku" in model_lower:
        # Claude 3.5 Haiku pricing (legacy)
        input_rate = 1.00
        output_rate = 5.00
        cache_write_rate = 1.25
        cache_read_rate = 0.10
    elif "sonnet" in model_lower:
        # Claude Sonnet 4 pricing
        input_rate = 3.00
        output_rate = 15.00
        cache_write_rate = 3.75
        cache_read_rate = 0.30
    else:
        # Default to Opus 4/4.1 pricing (most expensive, safe default)
        input_rate = 15.00
        output_rate = 75.00
        cache_write_rate = 18.75
        cache_read_rate = 1.50
    
    # ã‚³ã‚¹ãƒˆè¨ˆç®—ï¼ˆper million tokensï¼‰
    input_cost = (input_tokens / 1_000_000) * input_rate
    output_cost = (output_tokens / 1_000_000) * output_rate
    cache_write_cost = (cache_creation / 1_000_000) * cache_write_rate
    cache_read_cost = (cache_read / 1_000_000) * cache_read_rate
    
    total_cost = input_cost + output_cost + cache_write_cost + cache_read_cost
    
    return total_cost

def format_cost(cost):
    """Format cost for display"""
    if cost < 0.01:
        return f"${cost:.4f}"
    elif cost < 1:
        return f"${cost:.3f}"
    else:
        return f"${cost:.2f}"

# ========================================
# RESPONSIVE DISPLAY MODE FORMATTERS
# ========================================

def shorten_model_name(model):
    """ãƒ¢ãƒ‡ãƒ«åã‚’çŸ­ç¸®å½¢ã«å¤‰æ›"""
    model_lower = model.lower()
    if "opus" in model_lower:
        if "4.1" in model or "4.5" in model_lower:
            return "Op4.5" if "4.5" in model_lower else "Op4.1"
        return "Op4"
    elif "sonnet" in model_lower:
        return "Son4"
    elif "haiku" in model_lower:
        return "Hai"
    return model[:6]

def format_output_full(ctx):
    """Full mode (>= 72 chars): 4è¡Œãƒ»å…¨é …ç›®ãƒ»è£…é£¾ã‚ã‚Š

    Example:
    [Sonnet 4] | ğŸŒ¿ main M2 +1 | ğŸ“ statusline | ğŸ’¬ 254
    Compact: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–’â–’ [58%] 91.8K/160.0K â™»ï¸ 99%
    Session: â–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’ [25%] 1h15m/5h (08:00-13:00)
    Burn:    â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–‡â–†â–…â–„â–ƒâ–‚â– 14.0M tok
    """
    lines = []

    # Line 1: Model/Git/Dir/Messages
    if ctx['show_line1']:
        line1_parts = []
        line1_parts.append(f"{Colors.BRIGHT_YELLOW}[{ctx['model']}]{Colors.RESET}")

        if ctx['git_branch']:
            git_display = f"{Colors.BRIGHT_GREEN}ğŸŒ¿ {ctx['git_branch']}"
            if ctx['modified_files'] > 0:
                git_display += f" {Colors.BRIGHT_YELLOW}M{ctx['modified_files']}"
            if ctx['untracked_files'] > 0:
                git_display += f" {Colors.BRIGHT_CYAN}+{ctx['untracked_files']}"
            git_display += Colors.RESET
            line1_parts.append(git_display)

        line1_parts.append(f"{Colors.BRIGHT_CYAN}ğŸ“ {ctx['current_dir']}{Colors.RESET}")

        if ctx['active_files'] > 0:
            line1_parts.append(f"{Colors.BRIGHT_WHITE}ğŸ“ {ctx['active_files']}{Colors.RESET}")

        if ctx['total_messages'] > 0:
            line1_parts.append(f"{Colors.BRIGHT_CYAN}ğŸ’¬ {ctx['total_messages']}{Colors.RESET}")

        if ctx['lines_added'] > 0 or ctx['lines_removed'] > 0:
            line1_parts.append(f"{Colors.BRIGHT_GREEN}+{ctx['lines_added']}{Colors.RESET}/{Colors.BRIGHT_RED}-{ctx['lines_removed']}{Colors.RESET}")

        if ctx['error_count'] > 0:
            line1_parts.append(f"{Colors.BRIGHT_RED}âš ï¸ {ctx['error_count']}{Colors.RESET}")

        if ctx['task_status'] != 'idle':
            line1_parts.append(f"{Colors.BRIGHT_YELLOW}âš¡ {ctx['task_status']}{Colors.RESET}")

        if ctx['session_cost'] > 0:
            cost_color = Colors.BRIGHT_YELLOW if ctx['session_cost'] > 10 else Colors.BRIGHT_WHITE
            line1_parts.append(f"{cost_color}ğŸ’° {format_cost(ctx['session_cost'])}{Colors.RESET}")

        lines.append(" | ".join(line1_parts))

    # Line 2: Compact tokens
    if ctx['show_line2']:
        line2_parts = []
        percentage = ctx['percentage']
        compact_display = format_token_count(ctx['compact_tokens'])
        percentage_color = get_percentage_color(percentage)

        if percentage >= 85:
            title_color = f"{Colors.BG_RED}{Colors.BRIGHT_WHITE}{Colors.BOLD}"
            percentage_display = f"{Colors.BG_RED}{Colors.BRIGHT_WHITE}{Colors.BOLD}[{percentage}%]{Colors.RESET}"
            compact_label = f"{title_color}Compact:{Colors.RESET}"
        else:
            compact_label = f"{Colors.BRIGHT_CYAN}Compact:{Colors.RESET}"
            percentage_display = f"{percentage_color}{Colors.BOLD}[{percentage}%]{Colors.RESET}"

        line2_parts.append(compact_label)
        line2_parts.append(get_progress_bar(percentage, width=20))
        line2_parts.append(percentage_display)
        line2_parts.append(f"{Colors.BRIGHT_WHITE}{compact_display}/{format_token_count(ctx['compaction_threshold'])}{Colors.RESET}")

        if ctx['cache_ratio'] >= 50:
            line2_parts.append(f"{Colors.BRIGHT_GREEN}â™»ï¸ {int(ctx['cache_ratio'])}% cached{Colors.RESET}")

        lines.append(" ".join(line2_parts))

    # Line 3: Session time
    if ctx['show_line3'] and ctx['session_duration']:
        line3_parts = []
        line3_parts.append(f"{Colors.BRIGHT_CYAN}Session:{Colors.RESET}")
        line3_parts.append(get_progress_bar(ctx['block_progress'], width=20, show_current_segment=True))
        line3_parts.append(f"{Colors.BRIGHT_WHITE}[{int(ctx['block_progress'])}%]{Colors.RESET}")
        line3_parts.append(f"{Colors.BRIGHT_WHITE}{ctx['session_duration']}/5h{Colors.RESET}")

        if ctx['session_time_info']:
            line3_parts.append(ctx['session_time_info'])

        lines.append(" ".join(line3_parts))

    # Line 4: Burn rate
    if ctx['show_line4'] and ctx['burn_line']:
        lines.append(ctx['burn_line'])

    return lines

def format_output_compact(ctx):
    """Compact mode (55-71 chars): 4è¡Œãƒ»ãƒ©ãƒ™ãƒ«çŸ­ç¸®ãƒ»è£…é£¾å‰Šæ¸›

    Example:
    [Son4] main M2+1 statusline ğŸ’¬254
    C: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’ [58%] 91K/160K
    S: â–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–’â–’â–’ [25%] 1h15m/5h
    B: â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–‡â–†â–… 14M
    """
    lines = []

    # Line 1: Shortened model/git/dir
    if ctx['show_line1']:
        line1_parts = []
        short_model = shorten_model_name(ctx['model'])
        line1_parts.append(f"{Colors.BRIGHT_YELLOW}[{short_model}]{Colors.RESET}")

        if ctx['git_branch']:
            git_display = f"{Colors.BRIGHT_GREEN}{ctx['git_branch']}"
            if ctx['modified_files'] > 0:
                git_display += f" M{ctx['modified_files']}"
            if ctx['untracked_files'] > 0:
                git_display += f"+{ctx['untracked_files']}"
            git_display += Colors.RESET
            line1_parts.append(git_display)

        line1_parts.append(f"{Colors.BRIGHT_CYAN}{ctx['current_dir']}{Colors.RESET}")

        if ctx['total_messages'] > 0:
            line1_parts.append(f"{Colors.BRIGHT_CYAN}ğŸ’¬{ctx['total_messages']}{Colors.RESET}")

        lines.append(" ".join(line1_parts))

    # Line 2: Compact tokens (shortened)
    if ctx['show_line2']:
        percentage = ctx['percentage']
        compact_display = format_token_count_short(ctx['compact_tokens'])
        threshold_display = format_token_count_short(ctx['compaction_threshold'])
        percentage_color = get_percentage_color(percentage)

        line2 = f"{Colors.BRIGHT_CYAN}C:{Colors.RESET} {get_progress_bar(percentage, width=12)} "
        line2 += f"{percentage_color}[{percentage}%]{Colors.RESET} "
        line2 += f"{Colors.BRIGHT_WHITE}{compact_display}/{threshold_display}{Colors.RESET}"
        lines.append(line2)

    # Line 3: Session (shortened)
    if ctx['show_line3'] and ctx['session_duration']:
        line3 = f"{Colors.BRIGHT_CYAN}S:{Colors.RESET} {get_progress_bar(ctx['block_progress'], width=12)} "
        line3 += f"{Colors.BRIGHT_WHITE}[{int(ctx['block_progress'])}%]{Colors.RESET} "
        line3 += f"{Colors.BRIGHT_WHITE}{ctx['session_duration']}/5h{Colors.RESET}"
        lines.append(line3)

    # Line 4: Burn (shortened)
    if ctx['show_line4'] and ctx['burn_timeline']:
        sparkline = create_sparkline(ctx['burn_timeline'], width=12)
        tokens_display = format_token_count_short(ctx['block_tokens'])
        line4 = f"{Colors.BRIGHT_CYAN}B:{Colors.RESET} {sparkline} {Colors.BRIGHT_WHITE}{tokens_display}{Colors.RESET}"
        lines.append(line4)

    return lines

def format_output_tight(ctx):
    """Tight mode (45-54 chars): 4è¡Œç¶­æŒãƒ»ã•ã‚‰ã«çŸ­ç¸®

    Example:
    [Son4] main M1+5
    C: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [58%] 91K
    S: â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ [25%] 1h15m
    B: â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ 14M
    """
    lines = []

    # Line 1: Model, branch (ultra short)
    if ctx['show_line1']:
        line1_parts = []
        short_model = shorten_model_name(ctx['model'])
        line1_parts.append(f"{Colors.BRIGHT_YELLOW}[{short_model}]{Colors.RESET}")

        if ctx['git_branch']:
            git_display = f"{Colors.BRIGHT_GREEN}{ctx['git_branch']}"
            if ctx['modified_files'] > 0 or ctx['untracked_files'] > 0:
                git_display += f" M{ctx['modified_files']}+{ctx['untracked_files']}"
            git_display += Colors.RESET
            line1_parts.append(git_display)

        lines.append(" ".join(line1_parts))

    # Line 2: Compact tokens (ultra short)
    if ctx['show_line2']:
        percentage = ctx['percentage']
        compact_display = format_token_count_short(ctx['compact_tokens'])
        percentage_color = get_percentage_color(percentage)

        line2 = f"{Colors.BRIGHT_CYAN}C:{Colors.RESET} {get_progress_bar(percentage, width=8)} "
        line2 += f"{percentage_color}[{percentage}%]{Colors.RESET} {Colors.BRIGHT_WHITE}{compact_display}{Colors.RESET}"
        lines.append(line2)

    # Line 3: Session (ultra short)
    if ctx['show_line3'] and ctx['session_duration']:
        line3 = f"{Colors.BRIGHT_CYAN}S:{Colors.RESET} {get_progress_bar(ctx['block_progress'], width=8)} "
        line3 += f"{Colors.BRIGHT_WHITE}[{int(ctx['block_progress'])}%]{Colors.RESET} {Colors.BRIGHT_WHITE}{ctx['session_duration']}{Colors.RESET}"
        lines.append(line3)

    # Line 4: Burn (ultra short)
    if ctx['show_line4'] and ctx['burn_timeline']:
        sparkline = create_sparkline(ctx['burn_timeline'], width=8)
        tokens_display = format_token_count_short(ctx['block_tokens'])
        line4 = f"{Colors.BRIGHT_CYAN}B:{Colors.RESET} {sparkline} {Colors.BRIGHT_WHITE}{tokens_display}{Colors.RESET}"
        lines.append(line4)

    return lines

def format_output_minimal(ctx):
    """Minimal mode (< 45 chars): 4è¡Œç¶­æŒãƒ»æœ€å°é™

    Example:
    Son4 main
    C: â–ˆâ–ˆâ–ˆâ–ˆ 58%
    S: â–ˆâ–ˆâ–‘â–‘ 25%
    B: â–â–‚â–ƒâ–„ 14M
    """
    lines = []

    # Line 1
    if ctx['show_line1']:
        short_model = shorten_model_name(ctx['model'])
        line1 = f"{Colors.BRIGHT_YELLOW}{short_model}{Colors.RESET}"
        if ctx['git_branch']:
            line1 += f" {Colors.BRIGHT_GREEN}{ctx['git_branch']}{Colors.RESET}"
        lines.append(line1)

    # Line 2
    if ctx['show_line2']:
        percentage = ctx['percentage']
        percentage_color = get_percentage_color(percentage)
        line2 = f"{Colors.BRIGHT_CYAN}C:{Colors.RESET} {get_progress_bar(percentage, width=4)} {percentage_color}{percentage}%{Colors.RESET}"
        lines.append(line2)

    # Line 3
    if ctx['show_line3'] and ctx['session_duration']:
        line3 = f"{Colors.BRIGHT_CYAN}S:{Colors.RESET} {get_progress_bar(ctx['block_progress'], width=4)} {Colors.BRIGHT_WHITE}{int(ctx['block_progress'])}%{Colors.RESET}"
        lines.append(line3)

    # Line 4
    if ctx['show_line4'] and ctx['burn_timeline']:
        sparkline = create_sparkline(ctx['burn_timeline'], width=4)
        tokens_display = format_token_count_short(ctx['block_tokens'])
        line4 = f"{Colors.BRIGHT_CYAN}B:{Colors.RESET} {sparkline} {Colors.BRIGHT_WHITE}{tokens_display}{Colors.RESET}"
        lines.append(line4)

    return lines

def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Claude Code statusline with configurable output', add_help=False)
    parser.add_argument('--show', type=str, help='Lines to show: 1,2,3,4 or all (default: use config settings)')
    parser.add_argument('--help', action='store_true', help='Show help')

    # Initialize args with default values first
    args = argparse.Namespace(show=None, help=False)

    # Parse arguments, but don't exit on failure (for stdin compatibility)
    try:
        args, _ = parser.parse_known_args()
    except:
        # Keep the default args initialized above
        pass
    
    # Handle help
    if args.help:
        print("statusline.py - Claude Code Status Line")
        print("Usage:")
        print("  echo '{\"session_id\":\"...\"}' | statusline.py")
        print("  echo '{\"session_id\":\"...\"}' | statusline.py --show 1,2")
        print("  echo '{\"session_id\":\"...\"}' | statusline.py --show simple")
        print("  echo '{\"session_id\":\"...\"}' | statusline.py --show all")
        print()
        print("Options:")
        print("  --show 1,2,3,4    Show specific lines (comma-separated)")
        print("  --show simple     Show compact and session lines (2,3)")
        print("  --show all        Show all lines")
        print("  --help            Show this help")
        return
    
    # Override display settings based on --show argument
    global SHOW_LINE1, SHOW_LINE2, SHOW_LINE3, SHOW_LINE4
    if args.show:
        # Reset all to False first
        SHOW_LINE1 = SHOW_LINE2 = SHOW_LINE3 = SHOW_LINE4 = False
        
        if args.show.lower() == 'all':
            SHOW_LINE1 = SHOW_LINE2 = SHOW_LINE3 = SHOW_LINE4 = True
        elif args.show.lower() == 'simple':
            SHOW_LINE2 = SHOW_LINE3 = True  # Show lines 2,3 (compact and session)
        else:
            # Parse comma-separated line numbers
            try:
                lines = [int(x.strip()) for x in args.show.split(',')]
                if 1 in lines: SHOW_LINE1 = True
                if 2 in lines: SHOW_LINE2 = True
                if 3 in lines: SHOW_LINE3 = True
                if 4 in lines: SHOW_LINE4 = True
            except ValueError:
                print("Error: Invalid --show format. Use: 1,2,3,4, simple, or all", file=sys.stderr)
                return
    
    try:
        # Read JSON from stdin
        input_data = sys.stdin.read()
        if not input_data.strip():
            # No input provided - just exit silently
            return
        data = json.loads(input_data)

        # ========================================
        # API DATA EXTRACTION (Claude Code stdin)
        # ========================================
        api_cost = data.get('cost', {})
        api_context = data.get('context_window', {})

        # API provided values (use these instead of manual calculation where possible)
        api_total_cost = api_cost.get('total_cost_usd', 0)
        api_input_tokens = api_context.get('total_input_tokens', 0)
        api_output_tokens = api_context.get('total_output_tokens', 0)
        api_context_size = api_context.get('context_window_size', 200000)

        # Lines changed (v2.1.6+ feature)
        api_lines_added = api_cost.get('total_lines_added', 0)
        api_lines_removed = api_cost.get('total_lines_removed', 0)

        # Context window percentage (v2.1.6+ feature)
        # These are pre-calculated by Claude Code and more accurate than manual calculation
        api_used_percentage = api_context.get('used_percentage')  # v2.1.6+
        api_remaining_percentage = api_context.get('remaining_percentage')  # v2.1.6+

        # Dynamic compaction threshold (80% of context window)
        compaction_threshold = api_context_size * 0.8

        # Extract basic values
        model = data.get('model', {}).get('display_name', 'Unknown')
        
        workspace = data.get('workspace', {})
        current_dir = os.path.basename(workspace.get('current_dir', data.get('cwd', '.')))
        session_id = data.get('session_id') or data.get('sessionId')
        
        # Get git info
        git_branch, modified_files, untracked_files = get_git_info(
            workspace.get('current_dir', data.get('cwd', '.'))
        )
        
        # Get token usage
        total_tokens = 0
        error_count = 0
        user_messages = 0
        assistant_messages = 0
        input_tokens = 0
        output_tokens = 0
        cache_creation = 0
        cache_read = 0
        
        # 5æ™‚é–“ãƒ–ãƒ­ãƒƒã‚¯æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
        block_stats = None
        current_block = None  # åˆæœŸåŒ–ã—ã¦å¤‰æ•°ã‚¹ã‚³ãƒ¼ãƒ—å•é¡Œã‚’å›é¿
        if session_id:
            try:
                # å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ™‚ç³»åˆ—ã§èª­ã¿è¾¼ã¿
                all_messages = load_all_messages_chronologically()
                
                # 5æ™‚é–“ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¤œå‡º
                try:
                    blocks = detect_five_hour_blocks(all_messages)
                except Exception as e:
                    print(f"DEBUG: Error in detect_five_hour_blocks: {e}", file=sys.stderr)
                    blocks = []
                
                # ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒå«ã¾ã‚Œã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‚’ç‰¹å®š
                current_block = find_current_session_block(blocks, session_id)
                
                if current_block:
                    # ãƒ–ãƒ­ãƒƒã‚¯å…¨ä½“ã®çµ±è¨ˆã‚’è¨ˆç®—
                    try:
                        block_stats = calculate_block_statistics_with_deduplication(current_block, session_id)
                    except Exception:
                        block_stats = None
                elif blocks:
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€æ–°ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ç”¨
                    active_blocks = [b for b in blocks if b.get('is_active', False)]
                    if active_blocks:
                        current_block = active_blocks[-1]  # æœ€æ–°ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ–ãƒ­ãƒƒã‚¯
                        try:
                            block_stats = calculate_block_statistics_with_deduplication(current_block, session_id)
                        except Exception:
                            block_stats = None
                
                # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®š - Compactç”¨ã¯ç¾åœ¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã¿
                # Compact lineç”¨: ç¾åœ¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ï¼ˆblock_statsã®æœ‰ç„¡ã«é–¢ã‚ã‚‰ãšè¨ˆç®—ï¼‰
                # transcript_pathãŒæä¾›ã•ã‚Œã¦ã„ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°session_idã‹ã‚‰æ¢ã™
                transcript_path_str = data.get('transcript_path')
                if transcript_path_str:
                    transcript_file = Path(transcript_path_str)
                else:
                    transcript_file = find_session_transcript(session_id)

                if transcript_file and transcript_file.exists():
                    try:
                        (total_tokens, _, error_count, user_messages, assistant_messages,
                         input_tokens, output_tokens, cache_creation, cache_read) = calculate_tokens_from_transcript(transcript_file)
                    except Exception as e:
                        # Log error for debugging Compact freeze issue
                        with open(Path.home() / '.claude' / 'statusline-error.log', 'a') as f:
                            f.write(f"\n{datetime.now()}: Error calculating Compact tokens: {e}\n")
                            f.write(f"Transcript file: {transcript_file}\n")
                        # Use block_stats as fallback if available
                        if block_stats:
                            total_tokens = 0
                            user_messages = block_stats.get('user_messages', 0)
                            assistant_messages = block_stats.get('assistant_messages', 0)
                            error_count = block_stats.get('error_count', 0)
                        else:
                            total_tokens = 0
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: block_statsãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
                    if block_stats:
                        total_tokens = 0
                        user_messages = block_stats.get('user_messages', 0)
                        assistant_messages = block_stats.get('assistant_messages', 0)
                        error_count = block_stats.get('error_count', 0)
                        input_tokens = 0
                        output_tokens = 0
                        cache_creation = 0
                        cache_read = 0
            except Exception as e:

                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«æ–¹å¼
                # transcript_pathãŒæä¾›ã•ã‚Œã¦ã„ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°session_idã‹ã‚‰æ¢ã™
                transcript_path_str = data.get('transcript_path')
                if transcript_path_str:
                    transcript_file = Path(transcript_path_str)
                else:
                    transcript_file = find_session_transcript(session_id)

                if transcript_file and transcript_file.exists():
                    (total_tokens, _, error_count, user_messages, assistant_messages,
                     input_tokens, output_tokens, cache_creation, cache_read) = calculate_tokens_from_transcript(transcript_file)
        
        # Calculate percentage for Compact display (dynamic threshold)
        # Prefer API-provided percentage (v2.1.6+) for accuracy, fallback to manual calculation
        compact_tokens = total_tokens
        if api_used_percentage is not None:
            # Use Claude Code's pre-calculated percentage (more accurate)
            percentage = min(100, round(api_used_percentage))
        else:
            # Fallback: manual calculation for older Claude Code versions
            # NOTE: API tokens (total_input/output_tokens) are CUMULATIVE session totals,
            # NOT current context window usage. Must use transcript-calculated tokens.
            percentage = min(100, round((compact_tokens / compaction_threshold) * 100))
        
        # Get additional info
        active_files = len(workspace.get('active_files', []))
        task_status = data.get('task', {}).get('status', 'idle')
        current_time = get_time_info()
        # 5æ™‚é–“ãƒ–ãƒ­ãƒƒã‚¯æ™‚é–“è¨ˆç®—
        duration_seconds = None
        session_duration = None
        if block_stats:
            # ãƒ–ãƒ­ãƒƒã‚¯çµ±è¨ˆã‹ã‚‰æ™‚é–“æƒ…å ±ã‚’å–å¾—
            duration_seconds = block_stats['duration_seconds']
            
            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿æ–‡å­—åˆ—
            if duration_seconds < 60:
                session_duration = f"{int(duration_seconds)}s"
            elif duration_seconds < 3600:
                session_duration = f"{int(duration_seconds/60)}m"
            else:
                hours = int(duration_seconds/3600)
                minutes = int((duration_seconds % 3600) / 60)
                session_duration = f"{hours}h{minutes}m" if minutes > 0 else f"{hours}h"
        
        # Calculate cost - prefer API value, fallback to manual calculation
        if api_total_cost > 0:
            session_cost = api_total_cost
        else:
            # Fallback to manual calculation if API cost unavailable
            session_cost = calculate_cost(input_tokens, output_tokens, cache_creation, cache_read, model)
        
        # Format displays - use API tokens for Compact line
        token_display = format_token_count(compact_tokens)
        percentage_color = get_percentage_color(percentage)

        # ========================================
        # RESPONSIVE DISPLAY MODE SYSTEM
        # ========================================

        # Get terminal width and determine display mode
        terminal_width = get_terminal_width()
        display_mode = get_display_mode(terminal_width)

        # ç’°å¢ƒå¤‰æ•°ã§å¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰æŒ‡å®šï¼ˆãƒ†ã‚¹ãƒˆ/ãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        forced_mode = os.environ.get('STATUSLINE_DISPLAY_MODE')
        if forced_mode in ('full', 'compact', 'tight', 'minimal'):
            display_mode = forced_mode

        # å¾“æ¥ã®ç’°å¢ƒå¤‰æ•°ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
        output_mode = os.environ.get('STATUSLINE_MODE', 'multi')
        if output_mode == 'single':
            display_mode = 'minimal'

        # Calculate common values
        total_messages = user_messages + assistant_messages

        # Calculate cache ratio
        cache_ratio = 0
        if cache_read > 0 or cache_creation > 0:
            all_tokens = compact_tokens + cache_read + cache_creation
            cache_ratio = (cache_read / all_tokens * 100) if all_tokens > 0 else 0

        # Calculate block progress
        block_progress = 0
        if duration_seconds is not None:
            hours_elapsed = duration_seconds / 3600
            block_progress = (hours_elapsed % 5) / 5 * 100

        # Generate session time info
        session_time_info = ""
        if block_stats and duration_seconds is not None:
            try:
                start_time_utc = block_stats['start_time']
                start_time_local = convert_utc_to_local(start_time_utc)
                session_start_time = start_time_local.strftime("%H:%M")
                end_time_local = start_time_local + timedelta(hours=5)
                session_end_time = end_time_local.strftime("%H:%M")

                now_local = datetime.now()
                if now_local > end_time_local:
                    session_time_info = f"{Colors.BRIGHT_YELLOW}{current_time}{Colors.RESET} {Colors.BRIGHT_YELLOW}(ended at {session_end_time}){Colors.RESET}"
                else:
                    session_time_info = f"{Colors.BRIGHT_WHITE}{current_time}{Colors.RESET} {Colors.BRIGHT_GREEN}({session_start_time} to {session_end_time}){Colors.RESET}"
            except Exception:
                session_time_info = f"{Colors.BRIGHT_WHITE}{current_time}{Colors.RESET}"

        # Generate burn line and timeline for context
        burn_line = ""
        burn_timeline = []
        block_tokens = 0
        if SHOW_LINE4 and block_stats:
            session_data = {
                'total_tokens': block_stats['total_tokens'],
                'duration_seconds': duration_seconds if duration_seconds and duration_seconds > 0 else 1,
                'start_time': block_stats.get('start_time'),
                'efficiency_ratio': block_stats.get('efficiency_ratio', 0),
                'current_cost': session_cost
            }
            burn_line = get_burn_line(session_data, session_id, block_stats, current_block)
            burn_timeline = generate_real_burn_timeline(block_stats, current_block)
            block_tokens = block_stats.get('total_tokens', 0)

        # Build context dictionary for formatters
        ctx = {
            'model': model,
            'git_branch': git_branch,
            'modified_files': modified_files,
            'untracked_files': untracked_files,
            'current_dir': current_dir,
            'active_files': active_files,
            'total_messages': total_messages,
            'lines_added': api_lines_added,
            'lines_removed': api_lines_removed,
            'error_count': error_count,
            'task_status': task_status,
            'session_cost': session_cost,
            'compact_tokens': compact_tokens,
            'compaction_threshold': compaction_threshold,
            'percentage': percentage,
            'cache_ratio': cache_ratio,
            'session_duration': session_duration,
            'block_progress': block_progress,
            'session_time_info': session_time_info,
            'burn_line': burn_line,
            'burn_timeline': burn_timeline,
            'block_tokens': block_tokens,
            'show_line1': SHOW_LINE1,
            'show_line2': SHOW_LINE2,
            'show_line3': SHOW_LINE3,
            'show_line4': SHOW_LINE4,
        }

        # Select formatter based on display mode
        if display_mode == 'full':
            lines = format_output_full(ctx)
        elif display_mode == 'compact':
            lines = format_output_compact(ctx)
        elif display_mode == 'tight':
            lines = format_output_tight(ctx)
        else:  # minimal
            lines = format_output_minimal(ctx)

        # Output lines
        for line in lines:
            print(f"\033[0m\033[1;97m{line}\033[0m")
        
    except Exception as e:
        # Fallback status line on error
        print(f"{Colors.BRIGHT_RED}[Error]{Colors.RESET} . | 0 | 0%")
        print(f"{Colors.LIGHT_GRAY}Check ~/.claude/statusline-error.log{Colors.RESET}")
        
        # Debug logging
        with open(Path.home() / '.claude' / 'statusline-error.log', 'a') as f:
            f.write(f"{datetime.now()}: {e}\n")
            f.write(f"Input data: {locals().get('input_data', 'No input')}\n\n")

def calculate_tokens_since_time(start_time, session_id):
    """ğŸ“Š SESSION LINE SYSTEM: Calculate tokens for current session only
    
    Calculates tokens from session start time to now for the burn line display.
    This is SESSION scope, NOT block scope. Used for burn rate calculations.
    
    CRITICAL: This is for the Burn line, NOT the Compact line.
    
    Args:
        start_time: Session start time (from Session line display)
        session_id: Current session ID
    Returns:
        int: Session tokens for burn rate calculation
    """
    try:
        if not start_time or not session_id:
            return 0
        
        transcript_file = find_session_transcript(session_id)
        if not transcript_file:
            return 0
        
        # Normalize start_time to UTC for comparison
        start_time_utc = convert_local_to_utc(start_time)
        
        session_messages = []
        processed_hashes = set()  # For duplicate removal 
        
        with open(transcript_file, 'r') as f:
            for line in f:
                try:
                    data = json.loads(line.strip())
                    if not data:
                        continue
                    
                    # Remove duplicates: messageId + requestId
                    message_id = data.get('message', {}).get('id')
                    request_id = data.get('requestId')
                    if message_id and request_id:
                        unique_hash = f"{message_id}:{request_id}"
                        if unique_hash in processed_hashes:
                            continue  # Skip duplicate
                        processed_hashes.add(unique_hash)
                    
                    # Get message timestamp
                    msg_timestamp = data.get('timestamp')
                    if not msg_timestamp:
                        continue
                    
                    # Parse timestamp and normalize to UTC
                    if isinstance(msg_timestamp, str):
                        msg_time = datetime.fromisoformat(msg_timestamp.replace('Z', '+00:00'))
                        if msg_time.tzinfo is None:
                            msg_time = msg_time.replace(tzinfo=timezone.utc)
                        msg_time_utc = msg_time.astimezone(timezone.utc)
                    else:
                        continue
                    
                    # Only include messages from session start time onwards
                    if msg_time_utc >= start_time_utc:
                        # Check for any messages with usage data (not just assistant)
                        if data.get('message', {}).get('usage'):
                            session_messages.append(data)
                
                except (json.JSONDecodeError, ValueError, TypeError):
                    continue
        
        # Sum all usage from session messages (each message is individual usage)
        total_input_tokens = 0
        total_output_tokens = 0
        total_cache_creation = 0
        total_cache_read = 0
        
        for message in session_messages:
            usage = message.get('message', {}).get('usage', {})
            if usage:
                total_input_tokens += usage.get('input_tokens', 0)
                total_output_tokens += usage.get('output_tokens', 0)
                total_cache_creation += usage.get('cache_creation_input_tokens', 0)
                total_cache_read += usage.get('cache_read_input_tokens', 0)
        
        #  nonCacheTokens for display (like burn rate indicator)
        non_cache_tokens = total_input_tokens + total_output_tokens
        cache_tokens = total_cache_creation + total_cache_read
        total_with_cache = non_cache_tokens + cache_tokens
        
        # Return cache-included tokens (like )
        return total_with_cache  #  cache tokens in display
        
    except Exception:
        return 0

# REMOVED: calculate_true_session_cumulative() - unused function (replaced by calculate_tokens_since_time)

# REMOVED: get_session_cumulative_usage() - unused function (5th line display not implemented)

def get_burn_line(current_session_data=None, session_id=None, block_stats=None, current_block=None):
    """Generate burn line display (Line 4)

    Creates the Burn line showing session tokens and burn rate.
    Uses 5-hour block timeline data with 15-minute intervals (20 segments).

    Format: "Burn: 14.0M (Rate: 321.1K t/m) [sparkline]"
    
    Args:
        current_session_data: Session data with session tokens
        session_id: Current session ID for sparkline data
        block_stats: Block statistics with burn_timeline data
    Returns:
        str: Formatted burn line for display
    """
    try:
        # Calculate burn rate
        burn_rate = 0
        if current_session_data:
            recent_tokens = current_session_data.get('total_tokens', 0)
            duration = current_session_data.get('duration_seconds', 0)
            if duration > 0:
                burn_rate = (recent_tokens / duration) * 60
        
        
        # ğŸ“Š BURN LINE TOKENS: 5-hour window total (from block_stats)
        # ===========================================================
        # 
        # Use 5-hour window total from block statistics
        # This should be ~21M tokens as expected
        #
        block_total_tokens = block_stats.get('total_tokens', 0) if block_stats else 0
        
        # Format session tokens for display (short format for Burn line)
        tokens_formatted = format_token_count_short(block_total_tokens)
        burn_rate_formatted = format_token_count_short(int(burn_rate))
        
        # Generate 5-hour timeline sparkline from REAL message data ONLY
        if block_stats and 'start_time' in block_stats and current_block:
            burn_timeline = generate_real_burn_timeline(block_stats, current_block)
        else:
            burn_timeline = [0] * 20
        
        sparkline = create_sparkline(burn_timeline, width=20)
        
        return (f"{Colors.BRIGHT_CYAN}Burn:   {Colors.RESET} {sparkline} "
                f"{Colors.BRIGHT_WHITE}{tokens_formatted} token(w/cache){Colors.RESET}, Rate: {burn_rate_formatted} t/m")
        
    except Exception as e:
        print(f"DEBUG: Burn line error: {e}", file=sys.stderr)
        return f"{Colors.BRIGHT_CYAN}Burn:   {Colors.RESET} {Colors.BRIGHT_WHITE}ERROR{Colors.RESET}"
if __name__ == "__main__":
    main()